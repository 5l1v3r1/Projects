# -*- coding: utf-8 -*-

import glob
import os
import time
from hashlib import md5

import MySQLdb
import numpy as np
import pathos.multiprocessing as mp
from pathos.multiprocessing import Pool
from pyAudioAnalysis import audioTrainTest as aT

from config import *  # Set mysql parameter is a file called config.py: host=, user=, passwd=, database=

class class_stats:
    def __init__(self, confusion_matrix, i):
        self.confusion_matrix = confusion_matrix
        self.i = i

    def stats_eval(self):
        i = self.i
        confusion_matrix = self.confusion_matrix
        numrows = len(confusion_matrix)
        numcols = len(confusion_matrix[0])
        assert(numcols == numrows) #Sanity Check
        numclasses = numrows

        full_matrix_sum = sum([sum(k) for k in confusion_matrix])

        true_pos = confusion_matrix[i][i] #The diagonals represent the true positives for each class
        row_sum = sum(confusion_matrix[i])
        false_neg = row_sum - true_pos
        sum_column = 0
        for j in xrange(0, numclasses):
            sum_column = sum_column + confusion_matrix[j][i]
        false_pos = sum_column - true_pos
        true_neg = full_matrix_sum - true_pos - false_neg - false_pos

        accu = (true_pos + true_neg) / full_matrix_sum
        sens = true_pos / (true_pos + false_neg)
        spec = true_neg / (true_neg + false_pos)

        prec = true_pos / (true_pos + false_pos)
        recall = sens

        self.true_pos = true_pos
        self.true_neg = true_neg
        self.false_pos = false_pos
        self.false_neg = false_neg

        self.accu = accu
        self.sens = sens
        self.spec = spec
        self.prec = prec
        self.recall = recall

    def f_score(self, Beta=1):
        prec = self.prec
        recall = self.recall
        fscore = (1 + Beta**2) * (prec * recall) / (Beta**2 * prec + recall)
        return fscore



def do_division(a, b):
    if a == 0 and b == 0:
        return 0.0
    else:
        return float(a) / float(b)

def unshared_copy(inList):
    #https://stackoverflow.com/questions/1601269/how-to-make-a-completely-unshared-copy-of-a-complicated-list-deep-copy-is-not
    if isinstance(inList, list):
        return list( map(unshared_copy, inList) )
    return inList


class tester:
    def __init__(self, test_dirs, model_dir=os.getcwd(), modelName='Test', classifierType='gradientboosting', level=0.7,
                 verbose=False, num_threads=mp.cpu_count()):
        self.test_dirs = test_dirs
        self.model_dir = model_dir
        self.modelName = modelName
        self.classifierType = classifierType
        self.level = level
        self.verbose = verbose
        self.num_threads = num_threads

    def test_model(self):

        test_dirs = self.test_dirs
        model_dir = self.model_dir
        modelName = self.modelName
        classifierType = self.classifierType
        level = self.level
        num_threads = self.num_threads

        # Used to test an existing model against new samples;
        # Test directories should contain the same categories and be in the same order as the original training data, but should contain seperate samples
        # model_dir is the path to the model file generated by the training function
        # modelName is the name of the model file generated by the trainging function
        # classifierType is the ML method used and should be the same as the training method used. Should be one of: svm, knn, gradientboosting, randomforest, extratrees
        # certainty_threshold: Any results with confidence below this should be treated as indeterminate
        # Level is confidence threshold above which test results should be considered
        # When store_to_mySQL is set to True results will be pushed to mySQL table specified in config.py

        # The following sets up a new table in the given db to store information about each classification; The table will be given the same name as the model file; the table will be dropped initially if a table with the same name already exists.

        start_time = time.clock()

        table_setup = '(filename VARCHAR(128), class VARCHAR(128), identifiedCorrectly VARCHAR(128), confidence DOUBLE, identifiedAs VARCHAR(128), PRIMARY KEY (filename));'


        with MySQLdb.connect(host=host, user=user, passwd=passwd,
                             db=database) as cur:  # config is in config.py: see above

            try:
                cur.execute("DROP TABLE " + modelName)
            except:
                pass

            cur.execute("CREATE TABLE " + modelName + table_setup)

        os.chdir(test_dirs[0])
        for file in glob.glob(u"*.wav"):  # Iterate through each wave file in the directory
            Result, P, classNames = aT.fileClassification(file, os.path.join(model_dir, modelName),
                                                          classifierType)  # Test the file
            break

        if classNames == -1:
            raise Exception("Model file " + os.path.join(model_dir, modelName) + " not found!")

        num_cats = len(classNames)
        temp = []
        for j in xrange(0, num_cats):
            temp.append(0)
        confusion_matrix = []
        for k in xrange(0, num_cats):
            confusion_matrix.append(unshared_copy(temp))

        confidence_above_90 = unshared_copy(temp)
        correct_above_90 = unshared_copy(temp)
        total_num_samples = unshared_copy(temp)
        confidence_corrected_con_matrix = unshared_copy(confusion_matrix)
        file_objects = []
        for i in xrange(0, len(test_dirs)):  # Iterate through each test directory
            dir = test_dirs[i]
            os.chdir(dir)
            rootdir, correct_cat = os.path.split(dir)
            for file in glob.glob(u"*.wav"):  # Iterate through each wave file in the directory
                file_objects.append([os.path.join(dir, file), correct_cat])

        pros = Pool(num_threads)
        pros.map(self.test_file, file_objects)

        with MySQLdb.connect(host=host, user=user, passwd=passwd,
                             db=database) as cur:  # config is in config.py: see above
            cur.execute("SELECT * FROM %s;" % modelName)
            all_results = cur.fetchall()

        for entry in all_results:
            correct_cat = entry[1]
            confidence = float(entry[3])
            identified_correctly = bool(entry[2])
            Result = float(entry[4])

            indexes = [t for t, x in enumerate(classNames) if unicode(x) == unicode(correct_cat)]
            if not len(indexes):
                raise Exception(correct_cat + "is not a correctly named category for this model!")
            elif len(indexes) != 1:
                raise Exception(correct_cat + "matches multiple categories in the model file!")
            cat_index = indexes[0]
            total_num_samples[cat_index] += 1
            confusion_matrix[cat_index][int(Result)] += 1
            if confidence > level:
                confidence_corrected_con_matrix[cat_index][int(Result)] += 1
                confidence_above_90[cat_index] += 1
                if unicode(correct_cat) == unicode(classNames[int(Result)]):
                    assert (identified_correctly)
                    correct_above_90[cat_index] += 1

        acc_above_90 = map(do_division, correct_above_90, confidence_above_90)
        percent_desicive_samples = map(do_division, confidence_above_90, total_num_samples)

        print '\n', "Agregated Results: ", '\n'
        print classNames
        print "acc above ", level, ": ", acc_above_90
        print "percent samples above ", level, ": ", percent_desicive_samples
        print "total samples tested in each category: ", total_num_samples, '\n'
        print "confusion matrix:"
        aT.printConfusionMatrix(np.array(confusion_matrix), classNames)
        print "\n", "confidence adjusted confustion matrix:"
        aT.printConfusionMatrix(np.array(confidence_corrected_con_matrix), classNames)

        print '\n', "Processed ", sum(total_num_samples), " samples in ", time.clock() - start_time, " seconds."

        stats = [class_stats(confusion_matrix,i) for i in xrange(0,len(confusion_matrix))]

        for obj in stats:
            obj.stats_eval()

        return stats

    def test_file(self, file_object):

        model_dir = self.model_dir
        modelName = self.modelName
        classifierType = self.classifierType
        verbose = self.verbose
        file, correct_cat = file_object

        Result, P, classNames = aT.fileClassification(file, os.path.join(model_dir, modelName),
                                                      classifierType)  # Test the file

        if verbose:
            print '\n', file
            print Result
            print classNames
            print P, '\n'

        current_file_results = {
            'filename': unicode(md5(file.encode('utf-8')).hexdigest()),
            'class': correct_cat,
            'identifiedCorrectly': str(unicode(correct_cat) == unicode(classNames[int(Result)])).upper(),
            'confidence': str(max(P)),
            'identifiedAs': str(Result)
        }
        with MySQLdb.connect(host=host, user=user, passwd=passwd, db=database) as cur:
            insert_statement = "INSERT INTO " + modelName + " (filename, class, identifiedCorrectly, confidence, identifiedAs) VALUES (\'" + \
                               current_file_results['filename'] + "\', \'" + current_file_results[
                                   'class'] + '\', ' + current_file_results['identifiedCorrectly'] + ', ' + \
                               current_file_results['confidence'] + ', ' + current_file_results['identifiedAs'] + ");"
            try:
                cur.execute(insert_statement)
            except:
                pass
